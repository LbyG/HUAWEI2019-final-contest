{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'深': 0, '秦': 1, '京': 2, '海': 3, '成': 4, '南': 5, '杭': 6, '苏': 7, '松': 8}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from pretrainedmodels.models import bninception\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "random_seed = 4050\n",
    "config_batch_size = 16\n",
    "class_n = (9 + 10 + 26)\n",
    "output_n = 9\n",
    "num_epochs = 100\n",
    "feature_extract = True\n",
    "use_pretrained=True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "char_to_index = {\"深\":0, \"秦\":1, \"京\":2, \"海\":3, \"成\":4, \"南\":5, \"杭\":6, \"苏\":7, \"松\":8}\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarIdDataset(Dataset):\n",
    "    def __init__(self, data_list, weight = 229, height = 229):\n",
    "        self.data_list = data_list\n",
    "        self.weight = weight\n",
    "        self.height = height\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        img_path, label = self.data_list[index][\"image_path\"], self.data_list[index][\"label\"]\n",
    "        img = np.array(Image.open(img_path))\n",
    "        img = cv2.resize(img,(self.weight, self.height))\n",
    "        img = transforms.Compose([transforms.ToPILImage(),transforms.ToTensor()])(img)\n",
    "        y = np.zeros([class_n * output_n])\n",
    "        for i in range(len(label)):\n",
    "            y[i * class_n + label[i]] = 1\n",
    "        \n",
    "        return img, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = \"./data/train-data-label.txt\"\n",
    "image_file = \"./data/train-data\"\n",
    "data_list = []\n",
    "with open(label_file, 'r') as file_to_read:\n",
    "    while True:\n",
    "        lines = file_to_read.readline().strip() # 整行读取数据\n",
    "        if not lines:\n",
    "            break\n",
    "        lines = lines.split(\",  \")\n",
    "        image_path = os.path.join(image_file, lines[1])\n",
    "        label = [];\n",
    "        label.append(char_to_index[lines[0][0]])\n",
    "        for i in range(1, len(lines[0])):\n",
    "            if '0' <= lines[0][i] and lines[0][i] <= '9':\n",
    "                label.append(9 + ord(lines[0][i]) - ord('0'))\n",
    "            else:\n",
    "                label.append(9 + 10 + ord(lines[0][i]) - ord('A'))\n",
    "        data_list.append({\"image_path\": image_path, \"label\":label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 296)\n",
      "all_big_count =  16\n",
      "x_big_count =  [0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 29, 29, 29, 25, 23, 23, 28, 36, 35, 32, 17, 17, 22, 29, 29, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 30, 41, 45, 44, 39, 33, 28, 26, 25, 23, 21, 22, 26, 27, 28, 31, 37, 38, 43, 47, 49, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 35, 37, 39, 41, 0, 0, 0, 16, 0, 0, 0, 16, 0, 16, 20, 32, 30, 28, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 44, 44, 44, 44, 44, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 25, 30, 31, 31, 30, 25, 19, 19, 24, 26, 28, 28, 28, 24, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 31, 36, 40, 34, 22, 21, 17, 17, 16, 17, 17, 19, 22, 28, 39, 35, 30, 23, 0, 0, 0, 0, 0, 0, 0, 0, 44, 44, 44, 44, 44, 44, 0, 0, 0, 0, 0, 0, 0, 0, 19, 23, 22, 18, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 35, 37, 39, 41, 0, 0, 0, 16, 0, 16, 20, 25, 24, 23, 23, 32, 30, 28, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 21, 26, 22, 21, 21, 20, 20, 21, 21, 24, 25, 25, 31, 28, 24, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 0, 0]\n",
      "{'image_path': './data/train-data/2b02495a8701b118.jpg', 'label': [2, 41, 25, 23, 40, 17, 34, 25, 18]}\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pixel_threshold = 255 / 2\n",
    "width_threshold = 20\n",
    "img = Image.open(data_list[0][\"image_path\"])\n",
    "img = img.convert('L')\n",
    "img = np.array(img)\n",
    "print(img.shape)\n",
    "h,w = img.shape\n",
    "# make sure character is white\n",
    "small_count, big_count = 0, 0\n",
    "for i in range(h):\n",
    "    for j in range(w):\n",
    "        if img[i, j] < threshold:\n",
    "            small_count += 1\n",
    "        else:\n",
    "            big_count += 1\n",
    "if small_count < big_count:\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            img[i, j] = 255 - img[i, j]\n",
    "\n",
    "x_big_count = [0] * w\n",
    "all_big_count = 0\n",
    "for i in range(h):\n",
    "    for j in range(w):\n",
    "        if img[i, j] >= threshold:\n",
    "            x_big_count[j] += 1;\n",
    "            all_big_count += 1\n",
    "all_big_count //= w\n",
    "print(\"all_big_count = \", all_big_count)\n",
    "for j in range(w):\n",
    "    if x_big_count[j] < all_big_count:\n",
    "        x_big_count[j] = 0\n",
    "print(\"x_big_count = \", x_big_count)\n",
    "print(data_list[0])\n",
    "print(type(img))\n",
    "\n",
    "plt.figure(\"love\")\n",
    "plt.imshow(img[:, 5:35], cmap='gray') # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    outputs = model(inputs).double()\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    for i in range(output_n):\n",
    "                        _, preds = torch.max(outputs[:, i*class_n:(i+1)*class_n], 1)\n",
    "                        _, y = torch.max(labels[:, i*class_n:(i+1)*class_n], 1)\n",
    "                        running_corrects += torch.sum(preds == y)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            print(len(dataloaders[phase].dataset))\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list, val_data_list, _, _ = train_test_split(data_list, data_list, test_size=0.22, random_state=random_seed)\n",
    "train_gen = CarIdDataset(train_data_list)\n",
    "train_loader = DataLoader(train_gen,batch_size=config_batch_size,shuffle=True,pin_memory=True,num_workers=2)\n",
    "\n",
    "val_gen = CarIdDataset(val_data_list)\n",
    "val_loader = DataLoader(val_gen,batch_size=config_batch_size,shuffle=False,pin_memory=True,num_workers=2)\n",
    "dataloaders_dict = {\"train\":train_loader, \"val\":val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "3120\n",
      "train Loss: 0.3412 Acc: 0.2551\n",
      "880\n",
      "val Loss: 0.1788 Acc: 0.3420\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "3120\n",
      "train Loss: 0.1519 Acc: 0.3766\n",
      "880\n",
      "val Loss: 0.1314 Acc: 0.3545\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "3120\n",
      "train Loss: 0.1252 Acc: 0.4144\n",
      "880\n",
      "val Loss: 0.1173 Acc: 0.3750\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "3120\n",
      "train Loss: 0.1156 Acc: 0.4045\n",
      "880\n",
      "val Loss: 0.1111 Acc: 0.3716\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-3df6bc92eff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-676d014592e5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0;31m#running_corrects += torch.sum(preds == labels.data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "set_parameter_requires_grad(model_ft, feature_extract)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs,class_n * output_n)\n",
    "model_ft.to(device)\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
