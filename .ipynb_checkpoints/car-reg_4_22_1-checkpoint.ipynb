{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbyg/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'深': 0, '秦': 1, '京': 2, '海': 3, '成': 4, '南': 5, '杭': 6, '苏': 7, '松': 8}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from pretrainedmodels.models import bninception\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from preprocess import *\n",
    "\n",
    "img_w, img_h = 64, 64\n",
    "random_seed = 4050\n",
    "config_batch_size = 4\n",
    "class_n = (9 + 10 + 26)\n",
    "output_n = 9\n",
    "num_epochs = 100\n",
    "feature_extract = True\n",
    "use_pretrained=True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "char_to_index = {\"深\":0, \"秦\":1, \"京\":2, \"海\":3, \"成\":4, \"南\":5, \"杭\":6, \"苏\":7, \"松\":8}\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarIdDataset(Dataset):\n",
    "    def __init__(self, data_list, mode, weight = 229, height = 229):\n",
    "        self.data_list = data_list\n",
    "        self.mode =mode\n",
    "        self.weight = weight\n",
    "        self.height = height\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        img_path, label = self.data_list[index][\"image_path\"], self.data_list[index][\"label\"]\n",
    "        img = np.array(Image.open(img_path))\n",
    "        \n",
    "        h, w, _ = img.shape\n",
    "        M = cv2.getAffineTransform(self.data_list[index][\"pts\"][0], self.data_list[index][\"pts\"][1])\n",
    "        img_dst = cv2.warpAffine(img, M, (w, h))\n",
    "        \n",
    "        #print(\"================================\")\n",
    "        char_img_list = []\n",
    "        for [x, y] in self.data_list[index][\"char_segmentation\"]:\n",
    "            char_img = cv2.resize(img_dst[:, x:y, :], (img_w, img_h), interpolation=cv2.INTER_CUBIC)\n",
    "            augment_img = iaa.SomeOf(2, [\n",
    "                iaa.Affine(rotate=(-30, 30), shear=(-16, 16), translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}),\n",
    "                iaa.GaussianBlur(sigma=(0.0, 3.0)),\n",
    "                iaa.AdditiveGaussianNoise(scale=0.5*255),\n",
    "                iaa.Add((-40, 40), per_channel=0.5),\n",
    "                iaa.Sharpen(alpha=0.5),\n",
    "                iaa.CropAndPad(percent=(-0.25, 0.25)),\n",
    "            ])\n",
    "            if self.mode == \"train_val\":\n",
    "                char_img = augment_img.augment_image(char_img)\n",
    "            char_img = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])(char_img)\n",
    "            #print(\"type(char_img) = \", type(char_img))\n",
    "            #print(\"char_img.shape = \", char_img.shape)\n",
    "            char_img_list.append(char_img)\n",
    "        img = torch.stack(char_img_list,0)\n",
    "        #img = torch.cat(inputs=char_img_list, dimension=0)\n",
    "        #print(\"img.shape = \", img.shape)\n",
    "        \n",
    "        #img = cv2.resize(img,(self.weight, self.height))\n",
    "        #img = transforms.Compose([])(img)\n",
    "        #print(img.shape)\n",
    "        y = np.zeros((output_n, class_n))\n",
    "        for i in range(len(label)):\n",
    "            y[i, label[i]] = 1\n",
    "        \n",
    "        return img, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = \"./data/train-data-label.txt\"\n",
    "image_file = \"./data/train-data\"\n",
    "model_file = \"./model\"\n",
    "data_list = []\n",
    "with open(label_file, 'r') as file_to_read:\n",
    "    while True:\n",
    "        lines = file_to_read.readline().strip() # 整行读取数据\n",
    "        if not lines:\n",
    "            break\n",
    "        lines = lines.split(\",  \")\n",
    "        image_path = os.path.join(image_file, lines[1])\n",
    "        label = [];\n",
    "        label.append(char_to_index[lines[0][0]])\n",
    "        for i in range(1, len(lines[0])):\n",
    "            if '0' <= lines[0][i] and lines[0][i] <= '9':\n",
    "                label.append(9 + ord(lines[0][i]) - ord('0'))\n",
    "            else:\n",
    "                label.append(9 + 10 + ord(lines[0][i]) - ord('A'))\n",
    "        data_list.append({\"image_path\": image_path, \"label\":label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                inputs = torch.cat([inputs[i] for i in range(inputs.shape[0])], 0)\n",
    "                labels = torch.cat([labels[i] for i in range(labels.shape[0])], 0)\n",
    "                #print(\"type(inputs) = \", type(inputs))\n",
    "                #print(\"type(labels) = \", type(labels))\n",
    "                #print(\"inputs.shape = \", inputs.shape)\n",
    "                #print(\"labels.shape = \", labels.shape)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    outputs = model(inputs).double()\n",
    "                    outputs = outputs.squeeze()\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    _, y = torch.max(labels, 1)\n",
    "                    #print(\"preds = \", preds)\n",
    "                    #print(\"labels = \", labels)\n",
    "                    running_corrects += torch.sum(preds == y)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            #print(len(dataloaders[phase].dataset))\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                model_filename = model_file + os.sep + str(epoch) + \"checkpoint.pth.tar\"\n",
    "                #torch.save({\"state_dict\":best_model_wts}, model_filename)\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            model_filename = model_file + os.sep + \"last_checkpoint.pth.tar\"\n",
    "            torch.save({\"state_dict\":model.state_dict()}, model_filename)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    best_model_filename = model_file + os.sep + \"best_checkpoint.pth.tar\"\n",
    "    torch.save({\"state_dict\":best_model_wts}, best_model_filename)\n",
    "    final_model_filename = model_file + os.sep + \"final_checkpoint.pth.tar\"\n",
    "    torch.save({\"state_dict\":model.state_dict()}, final_model_filename)\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor count in range(len(data_list)):\\n    data = data_list[count]\\n    print(\"count = \", count, \"data = \", data)\\n    pts1, pts2, char_segmentation = preprocess(data[\"image_path\"])\\n    data[\"pts\"] = [pts1, pts2]\\n    data[\"char_segmentation\"] = char_segmentation\\n    \\n    data_pkl_path = data[\"image_path\"].replace(\"jpg\", \"pkl\").replace(\"train-data\", \"preprocess\")\\n    output = open(data_pkl_path, \\'wb\\')\\n    pickle.dump(data, output)\\n    output.close()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_list = data_list[:100]\n",
    "\"\"\"\n",
    "for count in range(len(data_list)):\n",
    "    data = data_list[count]\n",
    "    print(\"count = \", count, \"data = \", data)\n",
    "    pts1, pts2, char_segmentation = preprocess(data[\"image_path\"])\n",
    "    data[\"pts\"] = [pts1, pts2]\n",
    "    data[\"char_segmentation\"] = char_segmentation\n",
    "    \n",
    "    data_pkl_path = data[\"image_path\"].replace(\"jpg\", \"pkl\").replace(\"train-data\", \"preprocess\")\n",
    "    output = open(data_pkl_path, 'wb')\n",
    "    pickle.dump(data, output)\n",
    "    output.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "preprocess_data_list = []\n",
    "for data in data_list:\n",
    "    data_pkl_path = data[\"image_path\"].replace(\"jpg\", \"pkl\").replace(\"train-data\", \"preprocess\")\n",
    "    pkl_file = open(data_pkl_path, 'rb')\n",
    "    pkl_data = pickle.load(pkl_file)\n",
    "    preprocess_data_list.append(pkl_data)\n",
    "\n",
    "output = open('./data.pkl', 'wb')\n",
    "pickle.dump(preprocess_data_list, output)\n",
    "output.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for data in data_list:\n",
    "    flag = 0\n",
    "    for [x, y] in data[\"char_segmentation\"]:\n",
    "        if x == y:\n",
    "            flag = 1\n",
    "    if flag == 1:\n",
    "        print(data[\"char_segmentation\"])\n",
    "        pts1, pts2, char_segmentation = preprocess(data[\"image_path\"])\n",
    "        data[\"pts\"] = [pts1, pts2]\n",
    "        data[\"char_segmentation\"] = char_segmentation\n",
    "        print(data[\"char_segmentation\"])\n",
    "output = open('./data.pkl', 'wb')\n",
    "pickle.dump(data_list, output)\n",
    "output.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pkl_file = open('./data.pkl', 'rb')\n",
    "\n",
    "data_list = pickle.load(pkl_file)\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_batch_size = 32\n",
    "train_data_list, val_data_list, _, _ = train_test_split(data_list, data_list, test_size=0.2, random_state=random_seed)\n",
    "train_gen = CarIdDataset(train_data_list, \"train\")\n",
    "train_loader = DataLoader(train_gen,batch_size=config_batch_size,shuffle=True,pin_memory=True,num_workers=2)\n",
    "\n",
    "val_gen = CarIdDataset(val_data_list, \"val\")\n",
    "val_loader = DataLoader(val_gen,batch_size=config_batch_size,shuffle=False,pin_memory=True,num_workers=2)\n",
    "dataloaders_dict = {\"train\":train_loader, \"val\":val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            torch.nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            torch.nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            torch.nn.ReLU(),\n",
    "            #torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "          )\n",
    "        self.classify = nn.Sequential(\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(256, 45),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        adaptiveAvgPoolWidth = x.shape[2]\n",
    "        x = F.avg_pool2d(x, kernel_size=adaptiveAvgPoolWidth)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classify(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    alexnet_list = list (alexnet_model.state_dict().keys() )\\n    model_list = list(model.state_dict().keys() )\\n    for i in range(len(model_list)):\\n        if model_list[i][:8] == \"features\":\\n            model_dict[model_list[i]] = alexnet_dict[alexnet_list[i]]\\n    model.load_state_dict(model_dict)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_net():\n",
    "#    alexnet_model = models.alexnet(pretrained=True)\n",
    "#    alexnet_dict = alexnet_model.state_dict().copy()\n",
    "\n",
    "    model = Net()\n",
    "    model_dict = model.state_dict().copy()\n",
    "    return model\n",
    "\"\"\"\n",
    "    alexnet_list = list (alexnet_model.state_dict().keys() )\n",
    "    model_list = list(model.state_dict().keys() )\n",
    "    for i in range(len(model_list)):\n",
    "        if model_list[i][:8] == \"features\":\n",
    "            model_dict[model_list[i]] = alexnet_dict[alexnet_list[i]]\n",
    "    model.load_state_dict(model_dict)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 6.2208 Acc: 0.2753\n",
      "val Loss: 6.1504 Acc: 0.4213\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 6.1351 Acc: 0.3969\n",
      "val Loss: 6.0807 Acc: 0.7700\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 6.0657 Acc: 0.5672\n",
      "val Loss: 6.0100 Acc: 1.0837\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 6.0021 Acc: 0.7775\n",
      "val Loss: 5.9659 Acc: 1.4975\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 5.9417 Acc: 0.9731\n",
      "val Loss: 5.9049 Acc: 1.8075\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 5.8808 Acc: 1.1847\n",
      "val Loss: 5.8488 Acc: 2.0925\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 5.8220 Acc: 1.3909\n",
      "val Loss: 5.7853 Acc: 2.3600\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 5.7621 Acc: 1.5309\n",
      "val Loss: 5.7257 Acc: 2.6012\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 5.7010 Acc: 1.7259\n",
      "val Loss: 5.6625 Acc: 2.7700\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 5.6396 Acc: 1.9019\n",
      "val Loss: 5.5977 Acc: 2.9475\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 5.5770 Acc: 2.0741\n",
      "val Loss: 5.5360 Acc: 3.0737\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 5.5116 Acc: 2.1963\n",
      "val Loss: 5.4611 Acc: 3.2313\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 5.4454 Acc: 2.3466\n",
      "val Loss: 5.3942 Acc: 3.3338\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 5.3765 Acc: 2.4659\n",
      "val Loss: 5.3219 Acc: 3.5112\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 5.3066 Acc: 2.5663\n",
      "val Loss: 5.2633 Acc: 3.6338\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 5.2319 Acc: 2.6819\n",
      "val Loss: 5.1847 Acc: 3.6900\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 5.1572 Acc: 2.8062\n",
      "val Loss: 5.1052 Acc: 3.7462\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 5.0765 Acc: 2.8666\n",
      "val Loss: 5.0159 Acc: 3.8750\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 4.9937 Acc: 2.9559\n",
      "val Loss: 4.9357 Acc: 3.9225\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 4.9076 Acc: 3.0566\n",
      "val Loss: 4.8824 Acc: 4.0637\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 4.8174 Acc: 3.1556\n",
      "val Loss: 4.7612 Acc: 4.0462\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 4.7228 Acc: 3.2134\n",
      "val Loss: 4.6796 Acc: 4.2275\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 4.6229 Acc: 3.2784\n",
      "val Loss: 4.5540 Acc: 4.2313\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 4.5237 Acc: 3.3581\n",
      "val Loss: 4.4797 Acc: 4.3312\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 4.4146 Acc: 3.4178\n",
      "val Loss: 4.3609 Acc: 4.3400\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 4.3049 Acc: 3.4788\n",
      "val Loss: 4.2128 Acc: 4.3850\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-07cd59130993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-fdfddd510bf4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMP_STATUS_CHECK_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "set_parameter_requires_grad(model_ft, feature_extract)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs,class_n)\n",
    "model_ft.to(device)\n",
    "print(model_ft)\n",
    "model_ft = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    torch.nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    torch.nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(128, class_n, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    ")\n",
    "\"\"\"\n",
    "model_ft = get_net()\n",
    "#last_model_filename = model_file + os.sep + \"last_checkpoint.pth.tar\"\n",
    "#last_model = torch.load(last_model_filename)\n",
    "#model_ft.load_state_dict(last_model[\"state_dict\"])\n",
    "model_ft.to(device)\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(2, 3)\n",
    "print(a)\n",
    "b = torch.zeros(2, 3)\n",
    "print(b)\n",
    "c = torch.stack((a,b),0)\n",
    "print(c.shape)\n",
    "print(c[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
